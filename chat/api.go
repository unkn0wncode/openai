// Package chat provides a wrapper for the OpenAI Chat API.
package chat

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	openai "openai/internal"
	"openai/util"
	"slices"
	"strings"
	"time"
)

const (
	apiURL = openai.BaseAPI + "v1/chat/completions"

	RoleSystem   = "system"
	RoleUser     = "user"
	RoleAI       = "assistant"
	RoleFunction = "function"
	RoleTool     = "tool"
)

// supportedImageTypes is a list of supported image file extensions.
var supportedImageTypes = []string{"png", "jpeg", "jpg", "gif", "webp"}

// Configuration flag to force enable LogTripper
var ForceEnableLogTripper bool = false

// Request is the request body for the Chat API.
type Request struct {
	// required

	Model    string    `json:"model"`    // model name, such as "gpt-3.5-turbo"
	Messages []Message `json:"messages"` // previous messages, including "system" prompt, user input and whole history

	// optional

	// What sampling temperature to use, between 0 and 2.
	// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
	// We generally recommend altering this or top_p but not both.
	Temperature float64 `json:"temperature,omitempty"` // default 1

	// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// We generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p,omitempty"` // default 1

	// How many chat completion choices to generate for each input message.
	N int `json:"n,omitempty"`

	// An object specifying the format that the model must output.
	// Is encoded as {"type": "json_object"}, or {"type": "text"},
	// or {"type": "json_schema", "json_schema": ...}.
	ResponseFormat ResponseFormatStr `json:"response_format,omitempty"` // default "text"

	// If set, partial message deltas will be sent, like in ChatGPT.
	// Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"` // default false

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"` // default []

	// Deprecated: use MaxCompletionTokens instead.
	// The maximum number of tokens allowed for the generated answer.
	// By default, the number of tokens the model can return will be (4096 - prompt tokens).
	MaxTokens int `json:"max_tokens,omitempty"` // default 4096 - prompt tokens

	// The maximum number of tokens allowed for the generated answer.
	MaxCompletionTokens int `json:"max_completion_tokens,omitempty"`

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty float64 `json:"presence_penalty,omitempty"` // default 0

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"` // default 0

	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	//
	// Example:
	//  {"20185":-100,"9552":-100} // "20185" is "AI" and "9552" is " AI", use https://platform.openai.com/tokenizer
	LogitBias map[string]int `json:"logit_bias,omitempty"` // default {}

	// Function calls that AI can request to be executed. Not all models support this.
	// Default is empty slice.
	// Here only names of functions are specified and real payload will be added automatically based on names.
	// API field "functions" is deprecated and replaced with "tools" but we use this one for internal list of functions.
	Functions []string `json:"-"` // default []

	// One of: "none", "auto", {"name": "function_name"}.
	// "none" prohibits function calls, "auto" allows them at AI's discretion and {"name": "function_name"} forces the use of one specified function.
	// Deprecated: use "tool_choice" instead.
	// FunctionCall json.RawMessage `json:"function_call,omitempty"` // default "none" if "functions" is empty or "auto" if not

	// "none", "auto", or "function_name".
	// "none" prohibits function calls, "auto" allows them at AI's discretion and "function_name" forces the use of one specified function.
	// If function name is given, it will be encoded in a tool format like {"type": "function", "function": {"name": "function_name"}}.
	ToolChoice ToolChoiceOption `json:"tool_choice,omitempty"` // default "auto"

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user,omitempty"` // default ""

	// custom (not part of the API)

	// By default (false), function calls will be executed automatically and request will be repeated with the results.
	// If set to true, function calls will be returned in the response as encoded JSON and must be executed manually.
	ReturnFunctionCalls bool `json:"-"` // default false
}

// MarshalJSON implements json.Marshaler interface.
// Encodes functions into the request body based on their names.
// Passes model from OpenRouter's model field.
func (data Request) MarshalJSON() ([]byte, error) {
	// if there are no functions, don't encode the field at all
	if len(data.Functions) == 0 {
		type Alias Request
		return marshal(&struct {
			Functions struct{} `json:"-"`
			*Alias
		}{
			Alias: (*Alias)(&data),
		})
	}

	type Tool struct {
		Type     string       `json:"type"`
		Function FunctionCall `json:"function"`
	}

	// find functions by names
	var tools []Tool
	for _, name := range data.Functions {
		f, ok := funcCalls[name]
		if !ok {
			return nil, fmt.Errorf("function '%s' is not registered", name)
		}
		tools = append(tools, Tool{
			Type:     "function",
			Function: f,
		})
	}

	// encode actual functions into the request body
	type Alias Request
	return marshal(&struct {
		Tools []Tool `json:"tools"`
		*Alias
	}{
		Tools: tools,
		Alias: (*Alias)(&data),
	})
}

// ResponseFormatStr represents a format that the model must output.
// Should be one of:
//   - "text" (default, normal text)
//   - "json_object" (deprecated, output is valid JSON but no specific schema)
//   - JSON schema as a string (output will match schema which must follow supported rule subset)
//
// Is encoded as {"type": "json_object"}, or {"type": "text"},
// or {"type": "json_schema", "json_schema": ...}.
type ResponseFormatStr string

func (rfs ResponseFormatStr) MarshalJSON() ([]byte, error) {
	if rfs == "" {
		return nil, nil
	}

	rf := struct {
		Type   string          `json:"type,omitempty"`
		Schema json.RawMessage `json:"json_schema,omitempty"`
	}{
		Type: string(rfs),
	}
	switch rfs {
	case "text", "json_object":
	default:
		rf.Type = "json_schema"
		rf.Schema = []byte(rfs)
	}

	return marshal(rf)
}

// Message represents a message in API request or response.
type Message struct {
	Role    string `json:"role"`    // "system"/"user"/"assistant"/"function"
	Content string `json:"content"` // normally a string, but when images are included, it's an array of objects

	// The refusal message generated by the model.
	Refusal string `json:"refusal,omitempty"`

	// Images are included in Content if present.
	Images []Image `json:"-"`

	// Name is required for "function" role and then must contain function name, otherwise it's optional.
	// [a-zA-Z0-9_]{1,64}
	Name string `json:"name,omitempty"`

	// FunctionCall can only be present if Role is "assistant".
	// Deprecated: use ToolCalls instead.
	FunctionCall FunctionCallData `json:"function_call,omitempty"`

	// ToolCallID is required for "tool" role and then must contain ID found in ToolCall.
	ToolCallID string `json:"tool_call_id,omitempty"`

	// ToolCalls can only be present if Role is "assistant".
	ToolCalls []ToolCallData `json:"tool_calls,omitempty"`
}

type Image struct {
	URL    string `json:"image_url"` // URL or base64 in format "data:image/jpeg;base64,{base64_image}"
	Detail string `json:"detail"`    // "low"/"high", default "low"
}

// MarshalJSON implements json.Marshaler interface.
// Encodes chat message while ensuring that FunctionCall is not present if empty.
// If Images are present, they are encoded into Content array.
func (data Message) MarshalJSON() ([]byte, error) {
	type Alias Message
	b, err := marshal(&struct {
		*Alias
	}{
		Alias: (*Alias)(&data),
	})
	if err != nil {
		return nil, err
	}

	if len(data.Images) != 0 {
		type ImageElement struct {
			URL    string `json:"url,omitempty"`
			Detail string `json:"detail,omitempty"`
		}
		type ContentElement struct {
			Type  string       `json:"type"`
			Text  string       `json:"text,omitempty"`
			Image ImageElement `json:"image_url,omitempty"`
		}
		var content []ContentElement
		if data.Content != "" {
			content = append(content, ContentElement{
				Type: "text",
				Text: data.Content,
			})
		}
		for _, img := range data.Images {
			// check if URL contains a supported file extension
			addr := strings.ToLower(img.URL)
			isSupported := false
			for _, ext := range supportedImageTypes {
				if strings.Contains(addr, "."+ext) {
					isSupported = true
					break
				}
			}
			if !isSupported && !strings.HasPrefix(addr, "data:image/") {
				openai.LogStd.Printf(
					"Drop image URL '%s' due to lack of supported file extension",
					img.URL,
				)
				continue
			}

			content = append(content, ContentElement{
				Type:  "image_url",
				Image: ImageElement(img),
			})
		}

		contentB, err := marshal(content)
		if err != nil {
			return nil, err
		}

		plainContent, err := marshal(struct {
			Content string `json:"content"`
		}{
			Content: data.Content,
		})
		if err != nil {
			return nil, err
		}
		plainContent = bytes.Trim(plainContent, "{}\n")

		b = []byte(strings.ReplaceAll(
			string(b),
			string(plainContent),
			`"content":`+string(contentB),
		))
	}

	// delete empty FunctionCall if present
	return []byte(strings.ReplaceAll(string(b), `,"function_call":{}`, "")), nil
}

// response is the response body for the Chat Completion API.
type response struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int    `json:"created"` // Unix timestamp
	Model   string `json:"model"`
	Usage   struct {
		Prompt     int `json:"prompt_tokens"`
		Completion int `json:"completion_tokens"`
		Total      int `json:"total_tokens"`
	} `json:"usage"`
	Choices []struct {
		Message      Message `json:"message"`
		FinishReason string  `json:"finish_reason"` // stop/length/content_filter/null
		Index        int     `json:"index"`
	} `json:"choices"`
	Error struct {
		Message string `json:"message"`
		Type    string `json:"type"`
		Param   string `json:"param"`
		Code    string `json:"code"`
	} `json:"error"`
}

// countTokens returns the number of tokens in the request.
func (data Request) countTokens() int {
	dup := data
	dup.Messages = make([]Message, len(data.Messages))

	for i, msg := range data.Messages {
		newMsg := msg

		var images []Image
		for _, img := range msg.Images {
			if !strings.HasPrefix(img.URL, "data:image/") {
				images = append(images, img)
			}
		}
		newMsg.Images = images

		dup.Messages[i] = newMsg
	}

	b, err := marshal(dup)
	if err != nil {
		panic("failed to marshal request body: " + err.Error())
	}

	if err := openai.LoadTokenEncoders(); err != nil {
		panic("failed to load token encoders: " + err.Error())
	}

	return len(openai.TokenEncoderChat.Encode(string(b), nil, nil))
}

// PromptPrice returns approximate price of the request's input in USD.
// Mind that output is not included and is priced higher, but usually is much shorter than input.
// Returns zero if pricing for the model is not known.
func (data Request) PromptPrice() float64 {
	pricing, ok := ModelsData[data.Model]
	if !ok {
		openai.LogStd.Printf("No pricing for found model '%s'", data.Model)
		return 0
	}
	return float64(data.countTokens()) * pricing.PriceIn
}

func (data Request) contextTokenLimit() int {
	modelData, ok := ModelsData[data.Model]
	if !ok {
		return ModelsData[""].LimitContext
	}
	return modelData.LimitContext
}

func (data Request) outputTokenLimit() int {
	modelData, ok := ModelsData[data.Model]
	if !ok {
		return ModelsData[""].LimitOutput
	}
	return modelData.LimitOutput
}

// trimMessages cuts off the oldest messages if the request is too long.
func (data Request) trimMessages() []Message {
	hasSystemPrompt := len(data.Messages) > 0 && data.Messages[0].Role == RoleSystem
	minMessages := 1
	if hasSystemPrompt {
		minMessages = 2
	}

	messages := data.Messages
	for len(data.Messages) > minMessages && data.countTokens() > data.contextTokenLimit()-data.MaxTokens {
		messages = nil
		if hasSystemPrompt {
			messages = append(messages, data.Messages[0])
		}

		for i := minMessages; i < len(data.Messages); i++ {
			messages = append(messages, data.Messages[i])
		}

		data.Messages = messages
	}

	return messages
}

func (data Request) execute() (*response, error) {
	if data.Model == "" {
		data.Model = DefaultModel
	}

	// Trim messages if the request is too long
	data.Messages = data.trimMessages()
	inputTokens := data.countTokens()
	if inputTokens > data.contextTokenLimit() {
		return nil, fmt.Errorf("prompt is likely too long: ~%d tokens, max %d tokens", inputTokens, data.contextTokenLimit())
	}

	// Ensure MaxTokens is set for specific models
	if data.MaxTokens == 0 && data.Model == ModelGPT4Vision {
		data.MaxTokens = min(data.outputTokenLimit(), data.contextTokenLimit()-inputTokens)
	}

	b, err := marshal(data)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request body: %w", err)
	}

	var req *http.Request
	req, err = http.NewRequest(http.MethodPost, apiURL, bytes.NewBuffer(b))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}
	openai.AddHeaders(req)

	// Enable LogTripper if forced
	if ForceEnableLogTripper && openai.LogTripper.Log == nil {
		enableLogTripper()
	}

	var resp *http.Response
	var duration time.Duration

	err = util.Retry(func() error {
		startTime := time.Now()
		resp, err = openai.Cli.Do(req)
		duration = time.Since(startTime)
		if err != nil {
			return err
		}

		if resp.StatusCode == http.StatusOK {
			// Disable LogTripper if it was enabled due to error and not forced
			if openai.LogTripper.Log != nil && !ForceEnableLogTripper {
				disableLogTripper()
			}
			return nil
		}

		if resp.StatusCode != http.StatusOK {
			return handleBadRequest(resp, data.Model, duration)
		}

		// Handle other non-OK statuses
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf(
			"request (model %s) failed with status: %s, response body: %s",
			data.Model, resp.Status, string(body),
		)
	}, 3, 3*time.Second)
	if resp != nil && resp.Body != nil {
		defer resp.Body.Close()
	}

	if err != nil {
		return nil, fmt.Errorf("failed to send request: %w", err)
	}

	// Read the response body
	rb, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	var res response
	if err := json.Unmarshal(rb, &res); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	openai.Log.Printf(
		"Consumed OpenAI tokens: %d + %d = %d ($%f) on model '%s' in %s",
		res.Usage.Prompt, res.Usage.Completion,
		res.Usage.Total, res.Cost(), res.Model, duration,
	)

	return &res, nil
}

// handleBadRequest handles the case when the API returns a 400 Bad Request status.
// Logs the request duration and returns an error with the response body.
func handleBadRequest(resp *http.Response, model string, duration time.Duration) error {
	openai.Log.Printf("Chat request timing: %s", duration)
	body, _ := io.ReadAll(resp.Body)
	errMsg := fmt.Errorf(
		"request (model %s) failed with status: %s, response body: %s",
		model, resp.Status, string(body),
	)
	enableLogTripper()
	return errMsg
}

// enableLogTripper enables LogTripper for the API requests and logs that it's enabled.
func enableLogTripper() {
	openai.LogStd.Printf("Enable LogTripper")
	openai.LogTripper.Log = openai.LogStd
}

// disableLogTripper disables LogTripper for the API requests and logs that it's disabled.
func disableLogTripper() {
	openai.LogStd.Printf("Disable LogTripper")
	openai.LogTripper.Log = nil
}

// checkFirst checks if API response is valid,
// returns raw content or function call of first choice and error.
func (resp *response) checkFirst() (string, error) {
	if resp == nil {
		return "", fmt.Errorf("response is nil")
	}

	if resp.Error.Message != "" {
		return "", fmt.Errorf("got API error: %s", resp.Error.Message)
	}

	if len(resp.Choices) == 0 {
		return "", fmt.Errorf("no choices returned")
	}

	if resp.Choices[0].Message.Refusal != "" {
		return "", fmt.Errorf(
			"AI returned refusal: %s",
			resp.Choices[0].Message.Refusal,
		)
	}

	finishReason := resp.Choices[0].FinishReason
	content := resp.Choices[0].Message.Content
	expectedFinishReasons := []string{
		"",
		openai.FinishReasonStop,
		openai.FinishReasonFunctionCall,
		openai.FinishReasonToolCalls,
	}
	if !slices.Contains(expectedFinishReasons, finishReason) {
		return content, fmt.Errorf("got unexpected finish reason: %s", finishReason)
	}
	if content != "" {
		openai.Log.Println("OpenAI response:", content)
	}
	if resp.Choices[0].Message.FunctionCall.Name != "" {
		openai.Log.Printf(
			"OpenAI called function: %+v",
			resp.Choices[0].Message.FunctionCall,
		)
	}
	if len(resp.Choices[0].Message.ToolCalls) != 0 {
		var funcCalls []string
		for _, tc := range resp.Choices[0].Message.ToolCalls {
			funcCalls = append(funcCalls, fmt.Sprintf("%+v", tc.Function))
		}
		openai.Log.Printf(
			"OpenAI called functions:\n%s",
			strings.Join(funcCalls, "\n"),
		)
	}

	return content, nil
}

// Cost returns the resulting cost of the completed request in USD.
// Returns zero if pricing for the model is not known.
func (resp *response) Cost() float64 {
	pricing, ok := ModelsData[resp.Model]
	if !ok {
		openai.LogStd.Printf("No pricing for found model '%s'", resp.Model)
		return 0
	}
	return float64(resp.Usage.Prompt)*pricing.PriceIn + float64(resp.Usage.Completion)*pricing.PriceOut
}

func marshal(v interface{}) ([]byte, error) {
	buffer := &bytes.Buffer{}
	encoder := json.NewEncoder(buffer)
	encoder.SetEscapeHTML(false)

	if err := encoder.Encode(v); err != nil {
		return nil, err
	}

	return buffer.Bytes(), nil
}
