// Package chat provides a wrapper for the OpenAI Chat API.
package chat

import (
	"bytes"
	"encoding/json"
	"fmt"
	openai "openai/internal"
	"openai/tools"
	"strings"
)

// Request is the request body for the Chat API.
type Request struct {
	// required

	Model    string    `json:"model"`    // model name, such as "gpt-3.5-turbo"
	Messages []Message `json:"messages"` // previous messages, including "system" prompt, user input and whole history

	// optional

	// What sampling temperature to use, between 0 and 2.
	// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
	// We generally recommend altering this or top_p but not both.
	Temperature float64 `json:"temperature,omitempty"` // default 1

	// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// We generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p,omitempty"` // default 1

	// How many chat completion choices to generate for each input message.
	N int `json:"n,omitempty"`

	// An object specifying the format that the model must output.
	// Is encoded as {"type": "json_object"}, or {"type": "text"},
	// or {"type": "json_schema", "json_schema": ...}.
	ResponseFormat ResponseFormatStr `json:"response_format,omitempty"` // default "text"

	// If set, partial message deltas will be sent, like in ChatGPT.
	// Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"` // default false

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"` // default []

	// Deprecated: use MaxCompletionTokens instead.
	// The maximum number of tokens allowed for the generated answer.
	// By default, the number of tokens the model can return will be (4096 - prompt tokens).
	MaxTokens int `json:"max_tokens,omitempty"` // default 4096 - prompt tokens

	// The maximum number of tokens allowed for the generated answer.
	MaxCompletionTokens int `json:"max_completion_tokens,omitempty"`

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty float64 `json:"presence_penalty,omitempty"` // default 0

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"` // default 0

	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	//
	// Example:
	//  {"20185":-100,"9552":-100} // "20185" is "AI" and "9552" is " AI", use https://platform.openai.com/tokenizer
	LogitBias map[string]int `json:"logit_bias,omitempty"` // default {}

	// Function calls that AI can request to be executed. Not all models support this.
	// Default is empty slice.
	// Here only names of functions are specified and real payload will be added automatically based on names.
	// API field "functions" is deprecated and replaced with "tools" but we use this one for internal list of functions.
	Functions []string `json:"-"` // default []

	// One of: "none", "auto", {"name": "function_name"}.
	// "none" prohibits function calls, "auto" allows them at AI's discretion and {"name": "function_name"} forces the use of one specified function.
	// Deprecated: use "tool_choice" instead.
	// FunctionCall json.RawMessage `json:"function_call,omitempty"` // default "none" if "functions" is empty or "auto" if not

	// "none", "auto", or "function_name".
	// "none" prohibits function calls, "auto" allows them at AI's discretion and "function_name" forces the use of one specified function.
	// If function name is given, it will be encoded in a tool format like {"type": "function", "function": {"name": "function_name"}}.
	ToolChoice tools.ToolChoiceOption `json:"tool_choice,omitempty"` // default "auto"

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user,omitempty"` // default ""

	// custom (not part of the API)

	// By default (false), function calls will be executed automatically and request will be repeated with the results.
	// If set to true, function calls will be returned in the response as encoded JSON and must be executed manually.
	ReturnFunctionCalls bool `json:"-"` // default false
}

// MarshalJSON implements json.Marshaler interface.
// Encodes functions into the request body based on their names.
// Passes model from OpenRouter's model field.
func (data Request) MarshalJSON() ([]byte, error) {
	// if there are no functions, don't encode the field at all
	if len(data.Functions) == 0 {
		type Alias Request
		return marshal(&struct {
			Functions struct{} `json:"-"`
			*Alias
		}{
			Alias: (*Alias)(&data),
		})
	}

	type Tool struct {
		Type     string             `json:"type"`
		Function tools.FunctionCall `json:"function"`
	}

	// find functions by names
	var tools []Tool
	for _, name := range data.Functions {
		f, ok := funcCalls[name]
		if !ok {
			return nil, fmt.Errorf("function '%s' is not registered", name)
		}
		tools = append(tools, Tool{
			Type:     "function",
			Function: f,
		})
	}

	// encode actual functions into the request body
	type Alias Request
	return marshal(&struct {
		Tools []Tool `json:"tools"`
		*Alias
	}{
		Tools: tools,
		Alias: (*Alias)(&data),
	})
}

// ResponseFormatStr represents a format that the model must output.
// Should be one of:
//   - "text" (default, normal text)
//   - "json_object" (deprecated, output is valid JSON but no specific schema)
//   - JSON schema as a string (output will match schema which must follow supported rule subset)
//
// Is encoded as {"type": "json_object"}, or {"type": "text"},
// or {"type": "json_schema", "json_schema": ...}.
type ResponseFormatStr string

func (rfs ResponseFormatStr) MarshalJSON() ([]byte, error) {
	if rfs == "" {
		return nil, nil
	}

	rf := struct {
		Type   string          `json:"type,omitempty"`
		Schema json.RawMessage `json:"json_schema,omitempty"`
	}{
		Type: string(rfs),
	}
	switch rfs {
	case "text", "json_object":
	default:
		rf.Type = "json_schema"
		rf.Schema = []byte(rfs)
	}

	return marshal(rf)
}

// Message represents a message in API request or response.
type Message struct {
	Role    string `json:"role"`    // "system"/"user"/"assistant"/"function"
	Content string `json:"content"` // normally a string, but when images are included, it's an array of objects

	// The refusal message generated by the model.
	Refusal string `json:"refusal,omitempty"`

	// Images are included in Content if present.
	Images []Image `json:"-"`

	// Name is required for "function" role and then must contain function name, otherwise it's optional.
	// [a-zA-Z0-9_]{1,64}
	Name string `json:"name,omitempty"`

	// FunctionCall can only be present if Role is "assistant".
	// Deprecated: use ToolCalls instead.
	FunctionCall openai.FunctionCallData `json:"function_call,omitempty"`

	// ToolCallID is required for "tool" role and then must contain ID found in ToolCall.
	ToolCallID string `json:"tool_call_id,omitempty"`

	// ToolCalls can only be present if Role is "assistant".
	ToolCalls []openai.ToolCallData `json:"tool_calls,omitempty"`
}

type Image struct {
	URL    string `json:"image_url"` // URL or base64 in format "data:image/jpeg;base64,{base64_image}"
	Detail string `json:"detail"`    // "low"/"high", default "low"
}

// MarshalJSON implements json.Marshaler interface.
// Encodes chat message while ensuring that FunctionCall is not present if empty.
// If Images are present, they are encoded into Content array.
func (data Message) MarshalJSON() ([]byte, error) {
	type Alias Message
	b, err := marshal(&struct {
		*Alias
	}{
		Alias: (*Alias)(&data),
	})
	if err != nil {
		return nil, err
	}

	if len(data.Images) != 0 {
		type ImageElement struct {
			URL    string `json:"url,omitempty"`
			Detail string `json:"detail,omitempty"`
		}
		type ContentElement struct {
			Type  string       `json:"type"`
			Text  string       `json:"text,omitempty"`
			Image ImageElement `json:"image_url,omitempty"`
		}
		var content []ContentElement
		if data.Content != "" {
			content = append(content, ContentElement{
				Type: "text",
				Text: data.Content,
			})
		}
		for _, img := range data.Images {
			// check if URL contains a supported file extension
			addr := strings.ToLower(img.URL)
			isSupported := false
			for _, ext := range supportedImageTypes {
				if strings.Contains(addr, "."+ext) {
					isSupported = true
					break
				}
			}
			if !isSupported && !strings.HasPrefix(addr, "data:image/") {
				openai.LogStd.Printf(
					"Drop image URL '%s' due to lack of supported file extension",
					img.URL,
				)
				continue
			}

			content = append(content, ContentElement{
				Type:  "image_url",
				Image: ImageElement(img),
			})
		}

		contentB, err := marshal(content)
		if err != nil {
			return nil, err
		}

		plainContent, err := marshal(struct {
			Content string `json:"content"`
		}{
			Content: data.Content,
		})
		if err != nil {
			return nil, err
		}
		plainContent = bytes.Trim(plainContent, "{}\n")

		b = []byte(strings.ReplaceAll(
			string(b),
			string(plainContent),
			`"content":`+string(contentB),
		))
	}

	// delete empty FunctionCall if present
	return []byte(strings.ReplaceAll(string(b), `,"function_call":{}`, "")), nil
}

func marshal(v interface{}) ([]byte, error) {
	buffer := &bytes.Buffer{}
	encoder := json.NewEncoder(buffer)
	encoder.SetEscapeHTML(false)

	if err := encoder.Encode(v); err != nil {
		return nil, err
	}

	return buffer.Bytes(), nil
}
